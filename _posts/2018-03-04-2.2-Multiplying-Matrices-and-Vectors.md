---
bg: "craterLake_orig.jpg"
layout: post
mathjax: true
title: 2.2 Multiplying Matrices and Vectors
crawlertitle: "deep learning machine learning linear algebra python getting started numpy data sciences"
categories: posts
tags: ['linear-algebra', 'python', 'numpy', 'deep-learning-book']
author: hadrienj
jupyter: https://github.com/hadrienj/deepLearningBook-Notes/blob/master/2.1%20Scalars%2C%20Vectors%2C%20Matrices%20and%20Tensors.ipynb
---


```python
import numpy as np
import matplotlib.pyplot as plt
```


```python
# Plot parameters
plt.style.use('ggplot')
plt.rcParams['axes.facecolor']='w'

%pylab inline
pylab.rcParams['figure.figsize'] = (3, 3)
```

    Populating the interactive namespace from numpy and matplotlib



```python
# Avoid inaccurate floating values (for inverse matrices in dot product for instance)
# See https://stackoverflow.com/questions/24537791/numpy-matrix-inversion-rounding-errors
np.set_printoptions(suppress=True)
```

# 2.2 Multiplying Matrices and Vectors

The standard way to multiply matrices is not to multiply each element of one with each elements of the other (this is the element-wise product). The matrix product, also called **dot product**, is calculated as following:

<img src="../../assets/images/2.2/dotProduct.png" width="400" alt="dotProduct">

This implies that the number of columns of the first matrix must be equals to the number of rows of the second matrix. Thus, if the shape of the first matrix is ($m \times n$) the second matrix need to be of shape ($n \times x$).

### Example 1.

Multiplication of a matrix and a vector.

<div>
$$\boldsymbol{A} _{3,2} \times \boldsymbol{B} _{2, 1} = \boldsymbol{C} _{3, 1}$$
</div>

<div>
$$
\begin{bmatrix}
    A_{1,1} & A_{1,2} \\\\
    A_{2,1} & A_{2,2} \\\\
    A_{3,1} & A_{3,2}
\end{bmatrix}\times
\begin{bmatrix}
    B_{1,1} \\\\
    B_{2,1}
\end{bmatrix}=
\begin{bmatrix}
    A_{1,1}B_{1,1} + A_{1,2}B_{2,1} \\\\
    A_{2,1}B_{1,1} + A_{2,2}B_{2,1} \\\\
    A_{3,1}B_{1,1} + A_{3,2}B_{2,1}
\end{bmatrix}
$$
</div>

For instance

<div>
$$
\begin{bmatrix}
    1 & 2 \\\\
    3 & 4 \\\\
    5 & 6
\end{bmatrix}\times
\begin{bmatrix}
    2 \\\\
    4
\end{bmatrix}=
\begin{bmatrix}
    1 \times 2 + 2 \times 4 \\\\
    3 \times 2 + 4 \times 4 \\\\
    5 \times 2 + 6 \times 4
\end{bmatrix}
=
\begin{bmatrix}
    10 \\\\
    22 \\\\
    34
\end{bmatrix}
$$
</div>

#### The Numpy command 'dot' can be used to compute the matrix product (or dot product)

Let's try to reproduce the last exemple:


```python
A = np.array([[1, 2], [3, 4], [5, 6]])
A
```

<pre class='output'>
array([[1, 2],
       [3, 4],
       [5, 6]])
</pre>



```python
B = np.array([[2], [4]])
B
```

<pre class='output'>
array([[2],
       [4]])
</pre>



```python
C = np.dot(A, B)
C
```

<pre class='output'>
array([[10],
       [22],
       [34]])
</pre>


It is even easier to use the method `dot()` of Numpy arrays:


```python
C = A.dot(B)
C
```

<pre class='output'>
array([[10],
       [22],
       [34]])
</pre>


### Example 2.

Multiplication of two matrices.

<div>
$$\boldsymbol{A} _{4,3} \times \boldsymbol{B} _{3, 2} = \boldsymbol{C} _{4, 2}$$
</div>

<div>
$$
\begin{align*}
&\begin{bmatrix}
    A_{1,1} & A_{1,2} \\\\
    A_{2,1} & A_{2,2} \\\\
    A_{3,1} & A_{3,2} \\\\
    A_{4,1} & A_{4,2}
\end{bmatrix}\times
\begin{bmatrix}
    B_{1,1} & B_{1,2} \\\\
    B_{2,1} & B_{2,2}
\end{bmatrix}=\\\\
&\begin{bmatrix}
    A_{1,1}B_{1,1} + A_{1,2}B_{2,1} & A_{1,1}B_{1,2} + A_{1,2}B_{2,2} \\\\
    A_{2,1}B_{1,1} + A_{2,2}B_{2,1} & A_{2,1}B_{1,2} + A_{2,2}B_{2,2} \\\\
    A_{3,1}B_{1,1} + A_{3,2}B_{2,1} & A_{3,1}B_{1,2} + A_{3,2}B_{2,2} \\\\
    A_{4,1}B_{1,1} + A_{4,2}B_{2,1} & A_{4,1}B_{1,2} + A_{4,2}B_{2,2}
\end{bmatrix}
\end{align*}
$$
</div>

For instance

<div>
$$
\begin{align*}
&\begin{bmatrix}
    1 & 2 & 3 \\\\
    4 & 5 & 6 \\\\
    7 & 8 & 9 \\\\
    10 & 11 & 12
\end{bmatrix}\times
\begin{bmatrix}
    2 & 7 \\\\
    1 & 2 \\\\
    3 & 6
\end{bmatrix}=\\\\
&\begin{bmatrix}
    2 \times 1 + 1 \times 2 + 3 \times 3 & 7 \times 1 + 2 \times 2 + 6 \times 3 \\\\
    2 \times 4 + 1 \times 5 + 3 \times 6 & 7 \times 4 + 2 \times 5 + 6 \times 6 \\\\
    2 \times 7 + 1 \times 8 + 3 \times 9 & 7 \times 7 + 2 \times 8 + 6 \times 9 \\\\
    2 \times 10 + 1 \times 11 + 3 \times 12 & 7 \times 10 + 2 \times 11 + 6 \times 12
\end{bmatrix}\\\\
&=
\begin{bmatrix}
    13 & 29 \\\\
    31 & 74 \\\\
    49 & 119 \\\\
    67 & 164
\end{bmatrix}
\end{align*}
$$
</div>


```python
A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
A
```

<pre class='output'>
array([[ 1,  2,  3],
       [ 4,  5,  6],
       [ 7,  8,  9],
       [10, 11, 12]])
</pre>



```python
B = np.array([[2, 7], [1, 2], [3, 6]])
B
```

<pre class='output'>
array([[2, 7],
       [1, 2],
       [3, 6]])
</pre>



```python
C = np.dot(A, B)
C
```

<pre class='output'>
array([[ 13,  29],
       [ 31,  74],
       [ 49, 119],
       [ 67, 164]])
</pre>


It works!

#### So the dot product can be formalized like that:

<div>
$$
C_{i,j} = A_{i,k}B_{k,j} = \sum_{k}A_{i,k}B_{k,j}
$$
</div>

More detailed examples about the dot product can be found [here](https://www.mathsisfun.com/algebra/matrix-multiplying.html).

# Matrices mutliplication is distributive

<div>
$$\boldsymbol{A}(\boldsymbol{B}+\boldsymbol{C}) = \boldsymbol{AB}+\boldsymbol{AC}$$
</div>

### Example 3.

<div>
$$
\boldsymbol{A}=\begin{bmatrix}
    2 & 3 \\\\
    1 & 4 \\\\
    7 & 6
\end{bmatrix}, 
\boldsymbol{B}=\begin{bmatrix}
    5 \\\\
    2
\end{bmatrix}, 
\boldsymbol{C}=\begin{bmatrix}
    4 \\\\
    3
\end{bmatrix}
$$
</div>


<div>
$$
\begin{align*}
\boldsymbol{A}(\boldsymbol{B}+\boldsymbol{C})&=\begin{bmatrix}
    2 & 3 \\\\
    1 & 4 \\\\
    7 & 6
\end{bmatrix}\times
\left(\begin{bmatrix}
    5 \\\\
    2
\end{bmatrix}+
\begin{bmatrix}
    4 \\\\
    3
\end{bmatrix}\right)=
\begin{bmatrix}
    2 & 3 \\\\
    1 & 4 \\\\
    7 & 6
\end{bmatrix}\times
\begin{bmatrix}
    9 \\\\
    5
\end{bmatrix}\\\\
&=
\begin{bmatrix}
    2 \times 9 + 3 \times 5 \\\\
    1 \times 9 + 4 \times 5 \\\\
    7 \times 9 + 6 \times 5
\end{bmatrix}=
\begin{bmatrix}
    33 \\\\
    29 \\\\
    93
\end{bmatrix}
\end{align*}
$$
</div>

is equivalent to

<div>
$$
\begin{align*}
\boldsymbol{A}\boldsymbol{B}+\boldsymbol{A}\boldsymbol{C} &= \begin{bmatrix}
    2 & 3 \\\\
    1 & 4 \\\\
    7 & 6
\end{bmatrix}\times
\begin{bmatrix}
    5 \\\\
    2
\end{bmatrix}+
\begin{bmatrix}
    2 & 3 \\\\
    1 & 4 \\\\
    7 & 6
\end{bmatrix}\times
\begin{bmatrix}
    4 \\\\
    3
\end{bmatrix}\\\\
&=
\begin{bmatrix}
    2 \times 5 + 3 \times 2 \\\\
    1 \times 5 + 4 \times 2 \\\\
    7 \times 5 + 6 \times 2
\end{bmatrix}+
\begin{bmatrix}
    2 \times 4 + 3 \times 3 \\\\
    1 \times 4 + 4 \times 3 \\\\
    7 \times 4 + 6 \times 3
\end{bmatrix}\\\\
&=
\begin{bmatrix}
    16 \\\\
    13 \\\\
    47
\end{bmatrix}+
\begin{bmatrix}
    17 \\\\
    16 \\\\
    46
\end{bmatrix}=
\begin{bmatrix}
    33 \\\\
    29 \\\\
    93
\end{bmatrix}
\end{align*}
$$
</div>


```python
A = np.array([[2, 3], [1, 4], [7, 6]])
A
```

<pre class='output'>
array([[2, 3],
       [1, 4],
       [7, 6]])
</pre>



```python
B = np.array([[5], [2]])
B
```

<pre class='output'>
array([[5],
       [2]])
</pre>



```python
C = np.array([[4], [3]])
C
```

<pre class='output'>
array([[4],
       [3]])
</pre>


$\boldsymbol{A}(\boldsymbol{B}+\boldsymbol{C})$:


```python
D = np.dot(A, (B+C))
D
```

<pre class='output'>
array([[33],
       [29],
       [93]])
</pre>


is equivalent to

$\boldsymbol{AB}+\boldsymbol{AC}$:


```python
D = np.dot(A, B) + np.dot(A, C)
D
```

<pre class='output'>
array([[33],
       [29],
       [93]])
</pre>


# Matrices mutliplication is associative

<div>
$$\boldsymbol{A}(\boldsymbol{BC}) = \boldsymbol{AB}(\boldsymbol{C})$$
</div>



```python
A = np.array([[2, 3], [1, 4], [7, 6]])
A
```

<pre class='output'>
array([[2, 3],
       [1, 4],
       [7, 6]])
</pre>



```python
B = np.array([[5, 3], [2, 2]])
B
```

<pre class='output'>
array([[5, 3],
       [2, 2]])
</pre>


$\boldsymbol{A}(\boldsymbol{BC})$:



```python
D = A.dot(B.dot(C))
D
```

<pre class='output'>
array([[100],
       [ 85],
       [287]])
</pre>


$\boldsymbol{AB}(\boldsymbol{C})$:


```python
D = (A.dot(B)).dot(C)
D
```

<pre class='output'>
array([[100],
       [ 85],
       [287]])
</pre>


# Matrix multiplication is not commutative

<div>
$$\boldsymbol{AB} \neq \boldsymbol{BA}$$
</div>


```python
A = np.array([[2, 3], [6, 5]])
A
```

<pre class='output'>
array([[2, 3],
       [6, 5]])
</pre>



```python
B = np.array([[5, 3], [2, 2]])
B
```

<pre class='output'>
array([[5, 3],
       [2, 2]])
</pre>


$\boldsymbol{AB}$:


```python
AB = np.dot(A, B)
AB
```

<pre class='output'>
array([[16, 12],
       [40, 28]])
</pre>


$\boldsymbol{BA}$:


```python
BA = np.dot(B, A)
BA
```

<pre class='output'>
array([[28, 30],
       [16, 16]])
</pre>


## However vector multiplication is commutative

<div>
$$\boldsymbol{x^{ \text{T}}y} = \boldsymbol{y^{\text{T}}x} $$
</div>


```python
x = np.array([[2], [6]])
x
```

<pre class='output'>
array([[2],
       [6]])
</pre>



```python
y = np.array([[5], [2]])
y
```

<pre class='output'>
array([[5],
       [2]])
</pre>


$\boldsymbol{x^\text{T}y}$:


```python
x_ty = np.dot(x.T, y)
x_ty
```

<pre class='output'>
array([[22]])
</pre>


$\boldsymbol{y^\text{T}x}$:


```python
y_tx = np.dot(y.T, x)
y_tx
```

<pre class='output'>
array([[22]])
</pre>


## Simplification of the matrix product

<div>
$$(\boldsymbol{AB})^{\text{T}} = \boldsymbol{B^{\text{T}}A}^{\text{T}}$$
</div>


```python
A = np.array([[2, 3], [1, 4], [7, 6]])
A
```

<pre class='output'>
array([[2, 3],
       [1, 4],
       [7, 6]])
</pre>



```python
B = np.array([[5, 3], [2, 2]])
B
```

<pre class='output'>
array([[5, 3],
       [2, 2]])
</pre>


$(\boldsymbol{AB})^{\text{T}}$:


```python
AB_t = np.dot(A, B).T
AB_t
```

<pre class='output'>
array([[16, 13, 47],
       [12, 11, 33]])
</pre>


$\boldsymbol{B^{\text{T}}A}^{\text{T}}$:


```python
B_tA = np.dot(B.T, A.T)
B_tA
```

<pre class='output'>
array([[16, 13, 47],
       [12, 11, 33]])
</pre>


# System of linear equations

Matrices can be used to describe a system of linear equations of the form $\boldsymbol{Ax}=\boldsymbol{b}$. Here is a set of linear equations:

<div>
$$
A_{1,1}x_1 + A_{1,2}x_2 + A_{1,n}x_n = b_1 \\\\
A_{2,1}x_1 + A_{2,2}x_2 + A_{2,n}x_n = b_2 \\\\
\cdots \\\\
A_{m,1}x_1 + A_{m,2}x_2 + A_{m,n}x_n = b_n
$$
</div>

The left hand term can considered as the product of a matrix $\boldsymbol{A}$ containing weights for each variable ($n$ columns) and each equation ($m$ rows):

<div>
$$
\boldsymbol{A}=
\begin{bmatrix}
    A_{1,1} & A_{1,2} & \cdots & A_{1,n} \\\\
    A_{2,1} & A_{2,2} & \cdots & A_{2,n} \\\\
    \cdots & \cdots & \cdots & \cdots \\\\
    A_{m,1} & A_{m,2} & \cdots & A_{m,n}
\end{bmatrix}
$$
</div>

with a vector $\boldsymbol{x}$ containing the $n$ unknowns

<div>
$$
\boldsymbol{x}=
\begin{bmatrix}
    x_1 \\\\
    x_2 \\\\
    \cdots \\\\
    x_n
\end{bmatrix}
$$
</div>

The dot product of $\boldsymbol{A}$ and $\boldsymbol{x}$ gives a set of equations. Here is a simple example:

<img src="../../assets/images/2.2/equationSystem.png" width="400" alt="equationSystem">

We have well a set of two equations with two unknowns. So the number of rows of $\boldsymbol{A}$ gives the number of equations and the number of columns gives the number of unknowns.

The equation system can be wrote like that:

<div>
$$
\begin{bmatrix}
    A_{1,1} & A_{1,2} & \cdots & A_{1,n} \\\\
    A_{2,1} & A_{2,2} & \cdots & A_{2,n} \\\\
    \cdots & \cdots & \cdots & \cdots \\\\
    A_{m,1} & A_{m,2} & \cdots & A_{m,n}
\end{bmatrix}
\times
\begin{bmatrix}
    x_1 \\\\
    x_2 \\\\
    \cdots \\\\
    x_n
\end{bmatrix}
=
\begin{bmatrix}
    b_1 \\\\
    b_2 \\\\
    \cdots \\\\
    b_m
\end{bmatrix}
$$
</div>

Or simply:

<div>
$$\boldsymbol{Ax}=\boldsymbol{b}$$
</div>


### Example 4.

We will try to convert the common form of a linear equation:

<div>
$$y=ax+b$$
</div>

to the matrix form. If we want to keep the previous notation:

<div>
$$x_2=ax_1+b$$
</div>

Don't confuse the variable $x_1$ and $x_2$ with the vector $\boldsymbol{x}$. This vector contains actually all the variables of our equations. Here we have:

<div>
$$
\boldsymbol{x} =
\begin{bmatrix}
    x_1 \\
    x_2
\end{bmatrix}
$$
</div>

In this example we will use the following equation:

<div>
$$
\begin{align*}
&x_2=2x_1+1\\\\
\Leftrightarrow& 2x_1-x_2=-1
\end{align*}
$$
</div>

If we want to end up with this system when we multiply $\boldsymbol{A}$ and $\boldsymbol{x}$ we need $\boldsymbol{A}$ to be:

<div>
$$
\boldsymbol{A}=
\begin{bmatrix}
    2 & -1
\end{bmatrix}
$$
</div>

because

<div>
$$
\begin{bmatrix}
    2 & -1
\end{bmatrix}
\cdot
\begin{bmatrix}
    x_1 \\
    x_2
\end{bmatrix}
=
\begin{bmatrix}
2x_1-1x_2
\end{bmatrix}
$$
</div>

To complete the equation we have

<div>
$$
\boldsymbol{b}=
\begin{bmatrix}
    -1
\end{bmatrix}
$$
</div>

which gives

<div>
$$
\begin{bmatrix}
    2 & -1
\end{bmatrix}
\cdot
\begin{bmatrix}
    x_1 \\
    x_2
\end{bmatrix}
=
\begin{bmatrix}
    -1
\end{bmatrix}
$$
</div>

This system of equations is thus very simple and contains only 1 equation ($\boldsymbol{A}$ has 1 row) and 2 variables ($\boldsymbol{A}$ has 2 columns). 

To summarise $\boldsymbol{A}$ will be the matrix of dimensions $m\times n$ containing scalars multiplying these variables (here $x_1$ is multiplied by 2 and $x_2$ by -1). The vector $\boldsymbol{x}$ contains the variables $x_1$ and $x_2$. And the right hand term is the constant $\boldsymbol{b}$:

<div>
$$
\boldsymbol{A}=
\begin{bmatrix}
    2 & -1
\end{bmatrix}
$$
</div>

<div>
$$
\boldsymbol{x}=
\begin{bmatrix}
    x_1\\\\
    x_2
\end{bmatrix}
$$
</div>

<div>
$$
\boldsymbol{b}=
\begin{bmatrix}
    -1
\end{bmatrix}
$$
</div>

We can write this system

<div>
$$
\boldsymbol{Ax}=\boldsymbol{b}
$$
</div>

We will see in [the next chapter]() that this compact way of writing set of linear equations can be very usefull. It provide actually a way to solve the equations.

# References

[1] [Math is fun - Multiplying matrices](https://www.mathsisfun.com/algebra/matrix-multiplying.html)
