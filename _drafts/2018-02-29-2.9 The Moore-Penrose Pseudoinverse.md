---
bg: "coli.jpg"
layout: post
mathjax: true
title:  "2.9 The Moore-Penrose Pseudoinverse"
crawlertitle: "2.9 The Moore-Penrose Pseudoinverse"
summary: "2.9 The Moore-Penrose Pseudoinverse"
date:   2018-02-29 20:09:47 +0700
categories: posts
tags: ['linear-algebra', 'python', 'numpy', 'deep-learning-book']
author: hadrien
jupyter: https://github.com/hadrienj/deepLearningBook-Notes/blob/master/2.2%20Multiplying%20Matrices%20and%20Vectors.ipynb
---

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
```


```python
# Plot parameters
sns.set()
# Seven hls color palette
current_palette_7 = sns.color_palette("hls", 7)
sns.set_palette(current_palette_7)

%pylab inline
pylab.rcParams['figure.figsize'] = (4, 4)
plt.rcParams['xtick.major.size'] = 0
plt.rcParams['ytick.major.size'] = 0
# rcParams.keys()
```

    Populating the interactive namespace from numpy and matplotlib



```python
# Avoid inaccurate floating values (for inverse matrices in dot product for instance)
# See https://stackoverflow.com/questions/24537791/numpy-matrix-inversion-rounding-errors
np.set_printoptions(suppress=True)
```

# 2.9 The Moore-Penrose Pseudoinverse

We will see here a direct application of the SVD (see [2.8]()).

As we have seen in [2.3]() that the inverse of a matrix $\boldsymbol{A}$ can be used to solve the equation $\boldsymbol{Ax}=\boldsymbol{b}$:

$\boldsymbol{A}^{-1}\boldsymbol{Ax}=\boldsymbol{A}^{-1}\boldsymbol{b}$

$\boldsymbol{I}_n\boldsymbol{x}=\boldsymbol{A}^{-1}\boldsymbol{b}$

$\boldsymbol{x}=\boldsymbol{A}^{-1}\boldsymbol{b}$

But in the cases where the set of equation have 0 or many solutions the inverse cannot be found and the equation cannot be solved. The pseudoinverse is $\boldsymbol{A}$ such as:

$\boldsymbol{AB}\approx\boldsymbol{I_n}$ minimizing $\mid\boldsymbol{AB}-\boldsymbol{I_n}\mid_2$.

The following formula can be used to find the pseudoinverse:

$\boldsymbol{A}^+= \boldsymbol{Vs}^+\boldsymbol{U}^T$

with $\boldsymbol{U}$, $\boldsymbol{s}$ and $\boldsymbol{V}$ respectively the left singular vectors, the singular values and the right singular vectors of $\boldsymbol{A}$ (see the SVD in [2.8]()). $\boldsymbol{A}^+$ is the pseudoinverse of $\boldsymbol{A}$ and $\boldsymbol{s}^+$ the pseudoinverse of $\boldsymbol{s}$. We saw that $\boldsymbol{s}$ is a diagonal matrix and thus $\boldsymbol{s}^+$ can be calculated by taking the reciprocal of the non zero values of $\boldsymbol{s}$.

### Example 1.

Let's see how to implement that. We will create a non square matrix $\boldsymbol{A}$, calculate its singular value decomposition and its pseudoinverse.

$
\boldsymbol{A}=\begin{bmatrix}
    7 & 2\\\\
    3 & 4\\\\
    5 & 3
\end{bmatrix}
$


```python
A = np.array([[7, 2], [3, 4], [5, 3]])
U, s, V = np.linalg.svd(A)

s_plus = np.zeros((A.shape[0], A.shape[1])).T
s_plus[:s.shape[0], :s.shape[0]] = np.linalg.inv(np.diag(s))

A_plus = V.T.dot(s_plus).dot(U.T)
A_plus
```




    array([[ 0.16666667, -0.10606061,  0.03030303],
           [-0.16666667,  0.28787879,  0.06060606]])



We can now check with the `pinv` function from Numpy that the pseudoinverse is correct:


```python
np.linalg.pinv(A)
```




    array([[ 0.16666667, -0.10606061,  0.03030303],
           [-0.16666667,  0.28787879,  0.06060606]])



It looks good! We can now check that it is really the near inverse of $\boldsymbol{A}$. Since we know that

$\boldsymbol{A}^{-1}\boldsymbol{A}=\boldsymbol{I_n}$

with

$\boldsymbol{I_2}=\begin{bmatrix}
    1 & 0 \\\\
    0 & 1
\end{bmatrix}
$


```python
A_plus.dot(A)
```




    array([[ 1.,  0.],
           [ 0.,  1.]])



This is not bad! This is almost the identity matrix!

A difference with the real inverse is that $\boldsymbol{A}^+\boldsymbol{A}\approx\boldsymbol{I}$ but $\boldsymbol{A}\boldsymbol{A}^+\neq\boldsymbol{I}$

Another way of computing the pseudoinverse is to use this formula:

$
(\boldsymbol{A}^T\boldsymbol{A})^{-1}\boldsymbol{A}^T
$

The result is less acurate than the SVD method and Numpy `pinv` uses the SVD [2]. Here is an example from the same matrix $\boldsymbol{A}$:


```python
A_plus_1 = np.linalg.inv(A.T.dot(A)).dot(A.T)
A_plus_1
```




    array([[ 0.16666667, -0.10606061,  0.03030303],
           [-0.16666667,  0.28787879,  0.06060606]])



In this case the result is the same as with the SVD way.

## Using the pseudoinverse to solve a overdetermined system of linear equations

In general there is no solution to overdetermined systems (see [2.4]() ; [4]). In the following picture, there is no point at the intersection of the three lines corresponding to three equations:

<img src="images/overdeterminedSystem.png" width=200 heigth=200>

The pseudoinverse solve the system in the least square error perspective: it finds the solution that minimize the error. We will see this more explicitly with an example.

### Example 2.

For this example we will consider this set of three equation with two unknowns:

$
\begin{cases}
-2x_1 + 2 = x_2 \\\\
4x_1 + 8 = x_2 \\\\
-1x_1 + 2 = x_2
\end{cases}
\Leftrightarrow
\begin{cases}
-2x_1 - x_2 = -2 \\\\
4x_1 - x_2 = -8 \\\\
-1x_1 - x_2 = -2
\end{cases}
$

Let's see their graphical representation:


```python
x1 = np.linspace(-5, 5, 1000)
x2_1 = -2*x1 + 2
x2_2 = 4*x1 + 8
x2_3 = -1*x1 + 2

plt.plot(x1, x2_1, 'b')
plt.plot(x1, x2_2, 'g')
plt.plot(x1, x2_3, 'r')
plt.xlim(-2., 1)
plt.ylim(1, 5)
plt.show()
```


![png](output_14_0.png)


We actually see that there is no solution.

Putting this into the matrix form we have:

$
\boldsymbol{A}=
\begin{bmatrix}
    -2 & -1 \\\\
    4 & -1 \\\\
    -1 & -1
\end{bmatrix}
$, $
\boldsymbol{x}=
\begin{bmatrix}
    x_1 \\\\
    x_2
\end{bmatrix}
$ and $
\boldsymbol{b}=
\begin{bmatrix}
    -2 \\\\
    -8 \\\\
    -2
\end{bmatrix}
$

So we have:

$
\boldsymbol{Ax} = \boldsymbol{b}
\Leftrightarrow
\begin{bmatrix}
    -2 & -1 \\\\
    4 & -1 \\\\
    -1 & -1
\end{bmatrix}
\begin{bmatrix}
    x_1 \\\\
    x_2
\end{bmatrix}
=
\begin{bmatrix}
    -2 \\\\
    -8 \\\\
    -2
\end{bmatrix}
$

We will now calculate the pseudoinverse of $\boldsymbol{A}$:


```python
A = np.array([[-2, -1], [4, -1], [-1, -1]])
A_plus = np.linalg.pinv(A)
A_plus
```




    array([[-0.11290323,  0.17741935, -0.06451613],
           [-0.37096774, -0.27419355, -0.35483871]])



Now that we have calculated the pseudoinverse of $\boldsymbol{A}$:

$
\boldsymbol{A}^+=
\begin{bmatrix}
    -0.11290323 &  0.17741935 & -0.06451613 \\\\
    -0.37096774 & -0.27419355 & -0.35483871
\end{bmatrix}
$

we can use it to find $\boldsymbol{x}$:


```python
b = np.array([[-2], [-8], [-2]])
res = A_plus.dot(b)
res
```




    array([[-1.06451613],
           [ 3.64516129]])



So we have

$
\boldsymbol{A}^+
\boldsymbol{b}
=
\begin{bmatrix}
    x1 \\\\
    x2
\end{bmatrix}
=
\begin{bmatrix}
    -0.11290323 &  0.17741935 & -0.06451613 \\\\
    -0.37096774 & -0.27419355 & -0.35483871
\end{bmatrix}
\begin{bmatrix}
    -2 \\\\
    -8 \\\\
    -2
\end{bmatrix}=
\begin{bmatrix}
    -1.06451613 \\\\
    3.64516129
\end{bmatrix}
$

In our two dimensions, the coordinate of $\boldsymbol{x}$ are $\begin{bmatrix}
    -1.06451613 \\\\
    3.64516129
\end{bmatrix}$. Let's plot this point along with the equations lines:


```python
plt.plot(x1, x2_1, 'b')
plt.plot(x1, x2_2, 'g')
plt.plot(x1, x2_3, 'r')
plt.xlim(-2., 1)
plt.ylim(1, 5)

plt.scatter(res[0], res[1])

plt.show()
```


![png](output_20_0.png)


Maybe you would have expected the point being at the barycenter of the triangle [5]. This is not the case becase the equations are not scaled the same way. Actually the point is at the intersection of the three [symmedians](https://en.wikipedia.org/wiki/Symmedian) of the triangle.

### Example 3.

This method can also be used to fit a line to a set of points. Let's take the following data points:

<img src="images/dataPoints.png" width=300 heigth=300>

We have this set of $\boldsymbol{x}$ and $\boldsymbol{y}$ and we are looking for the line $y=mx+b$ that minimize the error. We can represent the data points with a matrix equations:

$
\boldsymbol{Ax} = \boldsymbol{b}
\Leftrightarrow
\begin{bmatrix}
    0 & 1 \\\\
    2 & 1 \\\\
    3 & 1 \\\\
    4 & 1
\end{bmatrix}
\begin{bmatrix}
    m \\\\
    b
\end{bmatrix}
=
\begin{bmatrix}
    2 \\\\
    0 \\\\
    5 \\\\
    3
\end{bmatrix}
$

Note that here the matrix $\boldsymbol{A}$ represents the values of the coefficients. The column of 1 correspond to the intercepts (without it the fit would have the constraint to cross the origin). It gives the following set of equations:

$
\begin{cases}
    m\cdot 0 + 1\cdot b = 2 \\\\
    m\cdot 2 + 1\cdot b = 0 \\\\
    m\cdot 3 + 1\cdot b = 5 \\\\
    m\cdot 4 + 1\cdot b = 3
\end{cases}
$

We have the set of equations $mx+b=y$. The ones are used to give back the intercept parameter. For instance, in the first equation corresponding to the first point we have well $x=0$ and $y=2$. This can be confusing because here the vector $\boldsymbol{x}$ correspond to the coefficients. This is because the problem is different from the other examples: we are looking for the coefficients of a line and not for $x$ and $y$ features. We kept this notation to indicate the similarity with the last examples.

So we will construct these matrix and try to use the pseudoinverse to find the equation of the line minimizing the error (difference between the line and the actual data points).

Let's start with the creation of the matrix of $\boldsymbol{A}$ and $\boldsymbol{y}$:


```python
A = np.array([[0, 1], [1, 1], [2, 1], [3, 1], [3, 1], [4, 1]])
y = np.array([[2], [4], [0], [2], [5], [3]])
```

We can now calculate the pseudoinverse of $\boldsymbol{A}$:


```python
A_plus = np.linalg.pinv(A)
```

and apply it to the result to find the coefficients:


```python
coefs = A_plus.dot(y)
coefs
```




    array([[ 0.21538462],
           [ 2.2       ]])



These are the parameters of the fit. The slope is $m=0.21538462$ and the intercept is $b=2.2$. We will plot the data points and the regression line: 


```python
x = np.linspace(-1, 5, 1000)
y = coefs[0]*x + coefs[1]

plt.plot(A[:, 0], b, '*')
plt.plot(x, y)
plt.xlim(-1., 6)
plt.ylim(-0.5, 5.5)

plt.show()
```


![png](output_29_0.png)


If you are not sure about the result. Just check it with another method. For instance, with R:

```r
a <- data.frame(x=c(0, 1, 2, 3, 3, 4),
                y=c(2, 4, 0, 2, 5, 3))

ggplot(data=a, aes(x=x, y=y)) +
  geom_point() +
  stat_smooth(method = "lm", col = "red") +
  xlim(-1, 5) +
  ylim(-1, 6)
```

outputs:

<img src="images/rLinearFit.png" width=400, height=400>

You can also do the fit with the Numpy `polyfit` to check the parameters:


```python
np.polyfit(A[:, 0], b, 1)
```




    array([[ 0.21538462],
           [ 2.2       ]])



That's good! We have seen how to use the pseudoinverse in order to solve a simple regression problem. Let's see with a more real case.

### Example 4.

To see the effect with more data points we can generate data (see [this nice blog post](https://mec560sbu.github.io/2016/08/29/Least_SQ_Fitting/) for other methods of fitting).

We will generate a column vector (see `reshape()` bellow) containing 100 points with random $x$ values and pseudo-random $y$ values. The function `seed()` from the [Numpy.random package](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.seed.html) is used to freeze the randomisation and be able to reproduce the results:


```python
np.random.seed(123)
x = 5*np.random.rand(100)
y = 2*x + 1 + np.random.randn(100)

x = x.reshape(100, 1)
y = y.reshape(100, 1)

print x.shape, y.shape
print x[:10, :]
```

    (100, 1) (100, 1)
    [[ 3.48234593]
     [ 1.43069667]
     [ 1.13425727]
     [ 2.75657385]
     [ 3.59734485]
     [ 2.1155323 ]
     [ 4.90382099]
     [ 3.42414869]
     [ 2.40465951]
     [ 1.96058759]]


We will create the matrix $\boldsymbol{A}$ from $\boldsymbol{x}$ by adding a column of ones exactly like we did in the example 3.


```python
A = np.hstack((x,np.ones(np.shape(x))))
A[:10]
```




    array([[ 3.48234593,  1.        ],
           [ 1.43069667,  1.        ],
           [ 1.13425727,  1.        ],
           [ 2.75657385,  1.        ],
           [ 3.59734485,  1.        ],
           [ 2.1155323 ,  1.        ],
           [ 4.90382099,  1.        ],
           [ 3.42414869,  1.        ],
           [ 2.40465951,  1.        ],
           [ 1.96058759,  1.        ]])



We can now find the pseudoinverse of $\boldsymbol{A}$ and calculate the coefficients of the regression line:


```python
A_plus = np.linalg.pinv(A)
coefs = A_plus.dot(y)
coefs
```




    array([[ 1.9461907 ],
           [ 1.16994745]])



We can finally draw the point and the regression line:


```python
x_line = np.linspace(0, 5, 1000)
y_line = coefs[0]*x_line + coefs[1]

plt.plot(x, y, '*')
plt.plot(x_line, y_line)
```




    [<matplotlib.lines.Line2D at 0x10aa27490>]




![png](output_40_1.png)


This is working!

# References

[1] [Sean Owen - Pseudoinverse intuition](https://www.quora.com/What-is-the-intuition-behind-pseudo-inverse-of-a-matrix)

[2] [Numpy - linalg.pinv](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html)

[3] [Using the Moore-Penrose Pseudoinverse to Solve Linear Equations](https://www.youtube.com/watch?v=5bxsxM2UTb4)

[4] [Overdetermined systems](https://en.wikipedia.org/wiki/Overdetermined_system)

[5] [Least square solution in the triangle center](https://math.stackexchange.com/questions/471812/is-the-least-squares-solution-to-an-overdetermined-system-a-triangle-center)

[6] [Symmedian](https://en.wikipedia.org/wiki/Symmedian)

[7] [Least square fitting](https://mec560sbu.github.io/2016/08/29/Least_SQ_Fitting/)

[8] [Numpy random seed](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.seed.html)


```python

```
